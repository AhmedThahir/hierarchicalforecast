{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reconciled Probabilistic Forecasts (PERM-BU)\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Nixtla/hierarchicalforecast/blob/main/nbs/examples/PERMBU_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "In this notebook we will explore probabilistic hierarchical sample reconciliation using the PERM-BU method.\n",
    "\n",
    "The PERM-BU method leverages empirical bottom-level marginal distributions with empirical copula functions (describing bottom-level dependencies) to generate the distribution of the aggregate-level distributions using the bottom-up reconciliation. The sample reordering technique from the PERM-BU method reinjects multivariate dependencies into independent bottom-level samples.\n",
    "\n",
    "[Taieb, Souhaib Ben and Taylor, James W and Hyndman, Rob J. (2017). Coherent probabilistic forecasts for hierarchical time series. International conference on machine learning ICML.](https://proceedings.mlr.press/v70/taieb17a.html)\n",
    "\n",
    "Table of Contents\n",
    "1.   [Installing/Importing Libraries](#cell-1)\n",
    "2.   [Reading Hierarchical Dataset](#cell-2)\n",
    "3.   [StatsForecast Base Predictions](#cell-3)\n",
    "3.   [PERMBU Probabilistic Reconciliation](#cell-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-1\"></a>\n",
    "## 1. Installing/Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install hierarchicalforecast\n",
    "!pip install numba statsforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast.core import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, ETS, Naive\n",
    "\n",
    "from hierarchicalforecast.utils import aggregate\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import PERMBU, BottomUp, MinTrace\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-2\"></a>\n",
    "## 2. Reading Hierarchical Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TourismSmall dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\n",
    "df = df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\n",
    "df.insert(0, 'Country', 'Australia')\n",
    "\n",
    "# Create hierarchical seires based on geographic levels and purpose\n",
    "# And Convert quarterly ds string to pd.datetime format\n",
    "hierarchy_levels = [['Country'],\n",
    "                    ['Country', 'State'], \n",
    "                    ['Country', 'Purpose'], \n",
    "                    ['Country', 'State', 'Region'], \n",
    "                    ['Country', 'State', 'Purpose'], \n",
    "                    ['Country', 'State', 'Region', 'Purpose']]\n",
    "\n",
    "Y_df, S, tags = aggregate(df=df, spec=hierarchy_levels)\n",
    "qs = Y_df['ds'].str.replace(r'(\\d+) (Q\\d)', r'\\1-\\2', regex=True)\n",
    "Y_df['ds'] = pd.PeriodIndex(qs, freq='Q').to_timestamp()\n",
    "Y_df = Y_df.reset_index()\n",
    "\n",
    "#-----------------------------------------------------------------------------#\n",
    "# Filter Australian States for small hierarchy structure\n",
    "filter_tags = tags['Country/State/Region']\n",
    "df = Y_df[Y_df['unique_id'].isin(filter_tags)].copy()\n",
    "df.insert(0, 'Country', 'Australia')\n",
    "df['State'] = df['unique_id'].apply(lambda x: x.split('/')[1]).copy()\n",
    "df['Region'] = df['unique_id'].apply(lambda x: x.split('/')[2]).copy()\n",
    "del df['unique_id']\n",
    "\n",
    "hierarchy_levels = [['Country'], \n",
    "                    ['Country', 'State'],\n",
    "                    ['Country', 'State', 'Region']]\n",
    "Y_df, S, tags = aggregate(df=df, spec=hierarchy_levels)\n",
    "Y_df = Y_df.reset_index()\n",
    "Y_df.unique_id = Y_df.unique_id.astype('category')\n",
    "Y_df.unique_id = Y_df.unique_id.cat.set_categories(S.index) # Extra careful!!!\n",
    "\n",
    "# Check dataset\n",
    "y_country = Y_df[Y_df.unique_id=='Australia'].y\n",
    "y_states = Y_df[Y_df['unique_id'].isin(tags['Country/State'])].y.values\n",
    "y_states = y_states.reshape(8, len(y_country))\n",
    "y_states = np.sum(y_states, axis=0)\n",
    "\n",
    "plt.plot(y_country, label='Country')\n",
    "plt.plot(y_states+1000, label='Country/State')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Split train/test sets\n",
    "Y_test_df  = Y_df.groupby('unique_id').tail(4)\n",
    "Y_train_df = Y_df.drop(Y_test_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-3\"></a>\n",
    "## 3. StatsForecast Base Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 4\n",
    "\n",
    "n_series = len(Y_train_df.unique_id.unique())\n",
    "n_time   = len(Y_train_df.ds.unique())\n",
    "\n",
    "unique_ids = S.index\n",
    "n_samples = n_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare StatsForecast model\n",
    "fcst = StatsForecast(df=Y_train_df,\n",
    "                     #models=[Naive()],\n",
    "                     models=[AutoARIMA(season_length=4)],\n",
    "                     freq='QS', n_jobs=-1)\n",
    "model_name = fcst.models[0].__class__.__name__\n",
    "\n",
    "# Insample Predictions\n",
    "_ = fcst.forecast(h=h, fitted=True) # store insample forecasts\n",
    "insample_fcsts_df = fcst.forecast_fitted_values().reset_index()\n",
    "y_insample = insample_fcsts_df['y'].values.reshape(n_series, n_time)\n",
    "y_hat_insample = insample_fcsts_df[model_name].values.reshape(n_series, n_time)\n",
    "\n",
    "# Outsample Prediction Distribution\n",
    "# Hacked normal variance from P(mu-1std<x<mu+1std) = 68.27\n",
    "fcsts_df = fcst.forecast(h=h, level=[68.27])\n",
    "fcsts_df['std'] = fcsts_df[f'{model_name}-hi-68.27'] - fcsts_df[model_name]\n",
    "\n",
    "y_hat_mean = fcsts_df[model_name].values.reshape(n_series, h)\n",
    "y_hat_std = fcsts_df['std'].values.reshape(n_series, h)\n",
    "\n",
    "print('y_mean.shape', y_hat_mean.shape)\n",
    "print('y_std.shape', y_hat_std.shape)\n",
    "print('y_insample.shape', y_insample.shape)\n",
    "print('y_hat_insample.shape', y_hat_insample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking ordering of unique_id index\n",
    "fcsts_df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 8\n",
    "\n",
    "# Create plot vectors\n",
    "empty = np.empty((n_series, n_time))\n",
    "empty[:] = np.nan\n",
    "y_hat_prediction = np.concatenate([empty, y_hat_mean], axis=1)\n",
    "\n",
    "# Validate insample predictions\n",
    "plt.plot(y_insample[idx,:], label='Insample True')\n",
    "plt.plot(y_hat_insample[idx,:], label='Insample Prediction')\n",
    "plt.title('Insample Predictions')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Validate median sample predictions (Try reducing sample variance)\n",
    "plt.plot(y_insample[idx,:], label='True')\n",
    "plt.plot(y_hat_prediction[idx,:], label='Sample mean')\n",
    "plt.title('Predictions')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-4\"></a>\n",
    "## 4. PERMBU Probabilistic Reconciliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrec = PERMBU()\n",
    "hier_samples = hrec.reconcile(S=S.values,\n",
    "                              y_insample=y_insample,\n",
    "                              y_hat_insample=y_hat_insample,\n",
    "                              y_hat_mean=y_hat_mean,\n",
    "                              y_hat_std=y_hat_std)\n",
    "\n",
    "hier_quantiles = np.quantile(hier_samples, q=[0.05, 0.5, 0.95], axis=2)\n",
    "hier_quantiles = np.transpose(hier_quantiles, (1,2,0))\n",
    "\n",
    "# Create plot vectors\n",
    "empty_q = np.empty((n_series, n_time, 3))\n",
    "empty_q[:] = np.nan\n",
    "hier_quantiles = np.concatenate([empty_q, hier_quantiles], axis=1)\n",
    "y_plot = Y_df.y.values\n",
    "y_plot = y_plot.reshape(n_series,n_time+h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 12\n",
    "\n",
    "plt.plot(y_plot[idx,:], label='True')\n",
    "plt.plot(hier_quantiles[idx, :, 0], label='q5')\n",
    "plt.plot(hier_quantiles[idx, :, 1], label='q50')\n",
    "plt.plot(hier_quantiles[idx, :, 2], label='q50')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hierarchicalforecast",
   "language": "python",
   "name": "hierarchicalforecast"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
